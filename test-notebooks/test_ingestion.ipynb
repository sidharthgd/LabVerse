{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd31e1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "\n",
    "# Add backend to path for imports\n",
    "sys.path.insert(0, str(Path.cwd().parent / 'backend'))\n",
    "\n",
    "from app.config import settings\n",
    "from app.services.embeddings import EmbeddingService\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d891543",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFileIngestion:\n",
    "    \"\"\"\n",
    "    Ingestion service for data files (CSV, Excel, JSON, Parquet, etc.)\n",
    "    Extracts metadata, generates LLM summaries, and creates embeddings\n",
    "    using the existing EmbeddingService\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.embedding_service = EmbeddingService()\n",
    "        self.openai_client = self.embedding_service.openai_client\n",
    "\n",
    "    def extract_file_metadata(self, filepath: Path) -> Optional[Dict[str, Any]]:\n",
    "        try:\n",
    "            file_extension = filepath.suffix.lower()\n",
    "\n",
    "            if file_extension == '.csv':\n",
    "                df = pd.read_csv(filepath)\n",
    "            elif file_extension in ['.xlsx', '.xls']:\n",
    "                df = pd.read_excel(filepath)\n",
    "            elif file_extension == '.parquet':\n",
    "                df = pd.read_parquet(filepath)\n",
    "            elif file_extension == '.json':\n",
    "                df = pd.read_json(filepath)\n",
    "            else:\n",
    "                print(f\"Unsupported file type: {file_extension}\")\n",
    "                return None\n",
    "\n",
    "            file_stats = filepath.stat()\n",
    "\n",
    "            column_info = {}\n",
    "            for col in df.columns:\n",
    "                column_info[col] = {\n",
    "                    'dtype': str(df[col].dtype),\n",
    "                    'null_count': int(df[col].isnull().sum()),\n",
    "                    'null_percentage': round(float(df[col].isnull().sum() / len(df) * 100), 2),\n",
    "                    'unique_count': int(df[col].nunique())\n",
    "                }\n",
    "                if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                    min_val = df[col].min()\n",
    "                    max_val = df[col].max()\n",
    "                    mean_val = df[col].mean()\n",
    "                    median_val = df[col].median()\n",
    "                    column_info[col]['min'] = round(float(min_val), 6) if not pd.isna(min_val) else None\n",
    "                    column_info[col]['max'] = round(float(max_val), 6) if not pd.isna(max_val) else None\n",
    "                    column_info[col]['mean'] = round(float(mean_val), 6) if not pd.isna(mean_val) else None\n",
    "                    column_info[col]['median'] = round(float(median_val), 6) if not pd.isna(median_val) else None\n",
    "\n",
    "            example_rows = df.head(3).to_dict(orient='records')\n",
    "\n",
    "            # Convert problematic types (Timestamps, NaN) to strings, keep primitives as-is\n",
    "            for row in example_rows:\n",
    "                for key, value in row.items():\n",
    "                    if pd.isna(value):\n",
    "                        row[key] = None  # None is JSON-serializable\n",
    "                    elif isinstance(value, (pd.Timestamp, datetime)):\n",
    "                        row[key] = value.isoformat()  # Convert Timestamp to string\n",
    "                    elif isinstance(value, (np.integer, np.floating)):\n",
    "                        # Convert numpy types to Python primitives\n",
    "                        row[key] = float(value) if isinstance(value, np.floating) else int(value)\n",
    "                    # int, float, str, bool stay as-is - they're JSON-serializable\n",
    "\n",
    "            metadata = {\n",
    "                'filename': filepath.name,\n",
    "                'filepath': str(filepath.absolute()),\n",
    "                'file_extension': file_extension,\n",
    "                'file_size_bytes': file_stats.st_size,  # int - keep as-is\n",
    "                'file_size_mb': round(file_stats.st_size / (1024 * 1024), 2),  # float - keep as-is\n",
    "                'modified_time': datetime.fromtimestamp(file_stats.st_mtime).isoformat(),  # datetime ‚Üí string\n",
    "                'row_count': len(df),  # int - keep as-is\n",
    "                'column_count': len(df.columns),  # int - keep as-is\n",
    "                'column_names': df.columns.tolist(),  # list - keep as list\n",
    "                'column_info': column_info,  # dict - keep as dict\n",
    "                'example_rows': example_rows,  # list of dicts - keep as list\n",
    "                'memory_usage_mb': round(df.memory_usage(deep=True).sum() / (1024 * 1024), 2)  # float - keep as-is\n",
    "            }\n",
    "\n",
    "            return metadata\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filepath}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def metadata_to_context_string(self, metadata: Dict[str, Any]) -> str:\n",
    "        # Convert entire metadata dict to JSON string\n",
    "        json_string = json.dumps(metadata, indent=2)\n",
    "        \n",
    "        # Print for debugging purposes\n",
    "        print(\"üìã Metadata JSON:\")\n",
    "        print(json_string)\n",
    "        \n",
    "        # Return the stringified JSON\n",
    "        return json_string\n",
    "\n",
    "    def generate_llm_summary(self, context_string: str) -> str:\n",
    "        prompt = f\"\"\"You are analyzing a dataset for a lab data repository. Given the following metadata, write a concise but comprehensive natural language summary (2-4 sentences) that would be optimal for semantic search and RAG (Retrieval Augmented Generation) applications.\n",
    "\n",
    "The summary should:\n",
    "1. Describe what this dataset contains and its purpose\n",
    "2. Highlight key columns and their characteristics (data types, ranges, uniqueness)\n",
    "3. Be context rich and contain information that is outlined in a way that is most suitable for your output to be vector embedded.\n",
    "\n",
    "Dataset Metadata:\n",
    "{context_string}\n",
    "\n",
    "Write only the summary, no additional commentary or any other text:\"\"\"\n",
    "\n",
    "        response = self.openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a data analyst expert at creating concise, searchable summaries of datasets for RAG applications.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=300\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    def embed_and_store(self, metadata: Dict[str, Any], namespace: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Generate LLM summary, create embedding, and store in Pinecone\n",
    "        \"\"\"\n",
    "        print(f\"ü§ñ Generating LLM summary for {metadata['filename']}...\")\n",
    "        context_string = self.metadata_to_context_string(metadata)\n",
    "        llm_summary = self.generate_llm_summary(context_string)\n",
    "        print(f\"‚úì Summary: {llm_summary}\")\n",
    "        \n",
    "        embedding, _ = self.embedding_service.embed_text(llm_summary)\n",
    "        \n",
    "        file_id = f\"file_{metadata['filename'].replace('.', '_')}_{hash(metadata['filepath'])}\"\n",
    "        \n",
    "        vector = {\n",
    "            \"id\": file_id,\n",
    "            \"values\": embedding,\n",
    "            \"metadata\": {\n",
    "                \"filename\": metadata['filename'],\n",
    "                \"filepath\": metadata['filepath'],\n",
    "                \"file_type\": metadata['file_extension'],\n",
    "                \"row_count\": metadata['row_count'],\n",
    "                \"column_count\": metadata['column_count'],\n",
    "                \"column_names\": ','.join(metadata['column_names']),\n",
    "                \"file_size_mb\": metadata['file_size_mb'],\n",
    "                \"llm_summary\": llm_summary,\n",
    "                \"summary_preview\": llm_summary[:500]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if self.embedding_service.index:\n",
    "            self.embedding_service.index.upsert(vectors=[vector], namespace=namespace or \"\")\n",
    "            print(f\"‚úì Stored embedding for {metadata['filename']} with ID: {file_id}\")\n",
    "        else:\n",
    "            print(\"‚ö† Pinecone not initialized, skipping vector storage\")\n",
    "        \n",
    "        return {\n",
    "            \"file_id\": file_id,\n",
    "            \"embedding_dim\": len(embedding),\n",
    "            \"context_length\": len(llm_summary),\n",
    "            \"llm_summary\": llm_summary,\n",
    "            \"stored_in_pinecone\": self.embedding_service.index is not None\n",
    "        }\n",
    "\n",
    "    def process_directory(self, directory_path: str, namespace: Optional[str] = None) -> List[Dict[str, Any]]:\n",
    "        directory = Path(directory_path)\n",
    "        if not directory.exists():\n",
    "            print(f\"Directory {directory_path} does not exist\")\n",
    "            return []\n",
    "        supported_extensions = ['.csv', '.xlsx', '.xls', '.parquet', '.json']\n",
    "        results = []\n",
    "        for filepath in directory.rglob('*'):\n",
    "            if filepath.is_file() and filepath.suffix.lower() in supported_extensions:\n",
    "                print(f\"\\n{'='*60}\")\n",
    "                print(f\"Processing: {filepath.name}\")\n",
    "                print(f\"{'='*60}\")\n",
    "\n",
    "                metadata = self.extract_file_metadata(filepath)\n",
    "                if metadata:\n",
    "                    embedding_info = self.embed_and_store(metadata, namespace)\n",
    "                    results.append({\n",
    "                        'filepath': str(filepath),\n",
    "                        'metadata': metadata,\n",
    "                        'embedding_info': embedding_info,\n",
    "                        'status': 'success'\n",
    "                    })\n",
    "                else:\n",
    "                    results.append({\n",
    "                        'filepath': str(filepath),\n",
    "                        'status': 'failed'\n",
    "                    })\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processed {len(results)} files\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "148ac19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä METADATA EXTRACTED:\n",
      "{'filename': 'labverse_experiments_5k.xlsx', 'filepath': '/Users/sid/Programming/LabVerse/test-notebooks/../test_dataset/labverse_experiments_5k.xlsx', 'file_extension': '.xlsx', 'file_size_bytes': 251637, 'file_size_mb': 0.24, 'modified_time': '2025-10-03T16:05:24.253615', 'row_count': 5000, 'column_count': 7, 'column_names': ['experiment_id', 'sample_date', 'sample_name', 'temperature', 'result', 'status', 'notes'], 'column_info': {'experiment_id': {'dtype': 'int64', 'null_count': 0, 'null_percentage': 0.0, 'unique_count': 5000, 'min': 1013.0, 'max': 11036.0, 'mean': 6016.9474, 'median': 6006.5}, 'sample_date': {'dtype': 'datetime64[ns]', 'null_count': 0, 'null_percentage': 0.0, 'unique_count': 965}, 'sample_name': {'dtype': 'object', 'null_count': 0, 'null_percentage': 0.0, 'unique_count': 24}, 'temperature': {'dtype': 'float64', 'null_count': 0, 'null_percentage': 0.0, 'unique_count': 645, 'min': 32.59, 'max': 52.69, 'mean': 37.051716, 'median': 37.02}, 'result': {'dtype': 'float64', 'null_count': 0, 'null_percentage': 0.0, 'unique_count': 1219, 'min': 45.71, 'max': 79.53, 'mean': 55.518344, 'median': 55.48}, 'status': {'dtype': 'object', 'null_count': 0, 'null_percentage': 0.0, 'unique_count': 4}, 'notes': {'dtype': 'object', 'null_count': 510, 'null_percentage': 10.2, 'unique_count': 3828}}, 'example_rows': [{'experiment_id': 1013, 'sample_date': '2024-06-01T00:00:00', 'sample_name': 'Epsilon', 'temperature': 37.4, 'result': 55.41, 'status': 'FAILURE', 'notes': 'Analysis recorded a strong correlation with catalyst concentration under high shear conditions. The assay estimated batch-to-batch variability across replicate samples. The assay noted consistent thermal stability during the late phase of the run.'}, {'experiment_id': 1016, 'sample_date': '2025-01-04T00:00:00', 'sample_name': 'Alpha', 'temperature': 38.26, 'result': 60.33, 'status': 'SUCCESS', 'notes': None}, {'experiment_id': 1018, 'sample_date': '2025-07-13T00:00:00', 'sample_name': 'Sigma', 'temperature': 36.83, 'result': 54.99, 'status': 'PENDING', 'notes': 'Analysis detected unexpected precipitation across replicate samples. The assay estimated elevated background noise across replicate samples.'}], 'memory_usage_mb': np.float64(1.65)}\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Test with a single file\n",
    "# Create an instance of the ingestion service\n",
    "ingestion = DataFileIngestion()\n",
    "\n",
    "# Test with a single file (update path to your actual file)\n",
    "test_file = Path(\"../test_dataset/labverse_experiments_5k.xlsx\")\n",
    "\n",
    "# Extract metadata\n",
    "metadata = ingestion.extract_file_metadata(test_file)\n",
    "\n",
    "print(\"üìä METADATA EXTRACTED:\")\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e5010ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ GENERATING LLM SUMMARY...\n",
      "üìã Metadata JSON:\n",
      "{\n",
      "  \"filename\": \"labverse_experiments_5k.xlsx\",\n",
      "  \"filepath\": \"/Users/sid/Programming/LabVerse/test-notebooks/../test_dataset/labverse_experiments_5k.xlsx\",\n",
      "  \"file_extension\": \".xlsx\",\n",
      "  \"file_size_bytes\": 251637,\n",
      "  \"file_size_mb\": 0.24,\n",
      "  \"modified_time\": \"2025-10-03T16:05:24.253615\",\n",
      "  \"row_count\": 5000,\n",
      "  \"column_count\": 7,\n",
      "  \"column_names\": [\n",
      "    \"experiment_id\",\n",
      "    \"sample_date\",\n",
      "    \"sample_name\",\n",
      "    \"temperature\",\n",
      "    \"result\",\n",
      "    \"status\",\n",
      "    \"notes\"\n",
      "  ],\n",
      "  \"column_info\": {\n",
      "    \"experiment_id\": {\n",
      "      \"dtype\": \"int64\",\n",
      "      \"null_count\": 0,\n",
      "      \"null_percentage\": 0.0,\n",
      "      \"unique_count\": 5000,\n",
      "      \"min\": 1013.0,\n",
      "      \"max\": 11036.0,\n",
      "      \"mean\": 6016.9474,\n",
      "      \"median\": 6006.5\n",
      "    },\n",
      "    \"sample_date\": {\n",
      "      \"dtype\": \"datetime64[ns]\",\n",
      "      \"null_count\": 0,\n",
      "      \"null_percentage\": 0.0,\n",
      "      \"unique_count\": 965\n",
      "    },\n",
      "    \"sample_name\": {\n",
      "      \"dtype\": \"object\",\n",
      "      \"null_count\": 0,\n",
      "      \"null_percentage\": 0.0,\n",
      "      \"unique_count\": 24\n",
      "    },\n",
      "    \"temperature\": {\n",
      "      \"dtype\": \"float64\",\n",
      "      \"null_count\": 0,\n",
      "      \"null_percentage\": 0.0,\n",
      "      \"unique_count\": 645,\n",
      "      \"min\": 32.59,\n",
      "      \"max\": 52.69,\n",
      "      \"mean\": 37.051716,\n",
      "      \"median\": 37.02\n",
      "    },\n",
      "    \"result\": {\n",
      "      \"dtype\": \"float64\",\n",
      "      \"null_count\": 0,\n",
      "      \"null_percentage\": 0.0,\n",
      "      \"unique_count\": 1219,\n",
      "      \"min\": 45.71,\n",
      "      \"max\": 79.53,\n",
      "      \"mean\": 55.518344,\n",
      "      \"median\": 55.48\n",
      "    },\n",
      "    \"status\": {\n",
      "      \"dtype\": \"object\",\n",
      "      \"null_count\": 0,\n",
      "      \"null_percentage\": 0.0,\n",
      "      \"unique_count\": 4\n",
      "    },\n",
      "    \"notes\": {\n",
      "      \"dtype\": \"object\",\n",
      "      \"null_count\": 510,\n",
      "      \"null_percentage\": 10.2,\n",
      "      \"unique_count\": 3828\n",
      "    }\n",
      "  },\n",
      "  \"example_rows\": [\n",
      "    {\n",
      "      \"experiment_id\": 1013,\n",
      "      \"sample_date\": \"2024-06-01T00:00:00\",\n",
      "      \"sample_name\": \"Epsilon\",\n",
      "      \"temperature\": 37.4,\n",
      "      \"result\": 55.41,\n",
      "      \"status\": \"FAILURE\",\n",
      "      \"notes\": \"Analysis recorded a strong correlation with catalyst concentration under high shear conditions. The assay estimated batch-to-batch variability across replicate samples. The assay noted consistent thermal stability during the late phase of the run.\"\n",
      "    },\n",
      "    {\n",
      "      \"experiment_id\": 1016,\n",
      "      \"sample_date\": \"2025-01-04T00:00:00\",\n",
      "      \"sample_name\": \"Alpha\",\n",
      "      \"temperature\": 38.26,\n",
      "      \"result\": 60.33,\n",
      "      \"status\": \"SUCCESS\",\n",
      "      \"notes\": null\n",
      "    },\n",
      "    {\n",
      "      \"experiment_id\": 1018,\n",
      "      \"sample_date\": \"2025-07-13T00:00:00\",\n",
      "      \"sample_name\": \"Sigma\",\n",
      "      \"temperature\": 36.83,\n",
      "      \"result\": 54.99,\n",
      "      \"status\": \"PENDING\",\n",
      "      \"notes\": \"Analysis detected unexpected precipitation across replicate samples. The assay estimated elevated background noise across replicate samples.\"\n",
      "    }\n",
      "  ],\n",
      "  \"memory_usage_mb\": 1.65\n",
      "}\n",
      "üìù LLM SUMMARY:\n",
      "The dataset \"labverse_experiments_5k.xlsx\" contains 5,000 rows of experimental data aimed at analyzing various lab samples, with key columns including \"experiment_id\" (unique integers ranging from 1013 to 11036), \"sample_date\" (datetime entries), \"sample_name\" (categorical with 24 unique values), \"temperature\" (float values between 32.59 and 52.69 degrees), and \"result\" (float values from 45.71 to 79.53). Additional columns include \"status\" (categorical with 4 unique statuses) and \"notes\" (textual, with 10.2% null entries), providing insights into experimental outcomes and conditions. This structured dataset is designed for comprehensive analysis of sample performance and experimental conditions in a laboratory setting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate LLM summary\n",
    "print(\"ü§ñ GENERATING LLM SUMMARY...\")\n",
    "context_string = ingestion.metadata_to_context_string(metadata)\n",
    "llm_summary = ingestion.generate_llm_summary(context_string)\n",
    "print(\"üìù LLM SUMMARY:\")\n",
    "print(llm_summary)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b6adc0",
   "metadata": {},
   "source": [
    "The dataset \"labverse_experiments_5k.xlsx\" contains 5,000 rows of experimental data aimed at analyzing various lab samples, with key columns including \"experiment_id\" (unique integers ranging from 1013 to 11036), \"sample_date\" (datetime entries), \"sample_name\" (categorical with 24 unique values), \"temperature\" (float values between 32.59 and 52.69 degrees), and \"result\" (float values from 45.71 to 79.53). Additional columns include \"status\" (categorical with 4 unique statuses) and \"notes\" (textual, with 10.2% null entries), providing insights into experimental outcomes and conditions. This structured dataset is designed for comprehensive analysis of sample performance and experimental conditions in a laboratory setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e12eba6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¢ EMBEDDING INFO:\n",
      "ü§ñ Generating LLM summary for labverse_experiments_5k.xlsx...\n",
      "üìã Metadata JSON:\n",
      "{\n",
      "  \"filename\": \"labverse_experiments_5k.xlsx\",\n",
      "  \"filepath\": \"/Users/sid/Programming/LabVerse/test-notebooks/../test_dataset/labverse_experiments_5k.xlsx\",\n",
      "  \"file_extension\": \".xlsx\",\n",
      "  \"file_size_bytes\": 251637,\n",
      "  \"file_size_mb\": 0.24,\n",
      "  \"modified_time\": \"2025-10-03T16:05:24.253615\",\n",
      "  \"row_count\": 5000,\n",
      "  \"column_count\": 7,\n",
      "  \"column_names\": [\n",
      "    \"experiment_id\",\n",
      "    \"sample_date\",\n",
      "    \"sample_name\",\n",
      "    \"temperature\",\n",
      "    \"result\",\n",
      "    \"status\",\n",
      "    \"notes\"\n",
      "  ],\n",
      "  \"column_info\": {\n",
      "    \"experiment_id\": {\n",
      "      \"dtype\": \"int64\",\n",
      "      \"null_count\": 0,\n",
      "      \"null_percentage\": 0.0,\n",
      "      \"unique_count\": 5000,\n",
      "      \"min\": 1013.0,\n",
      "      \"max\": 11036.0,\n",
      "      \"mean\": 6016.9474,\n",
      "      \"median\": 6006.5\n",
      "    },\n",
      "    \"sample_date\": {\n",
      "      \"dtype\": \"datetime64[ns]\",\n",
      "      \"null_count\": 0,\n",
      "      \"null_percentage\": 0.0,\n",
      "      \"unique_count\": 965\n",
      "    },\n",
      "    \"sample_name\": {\n",
      "      \"dtype\": \"object\",\n",
      "      \"null_count\": 0,\n",
      "      \"null_percentage\": 0.0,\n",
      "      \"unique_count\": 24\n",
      "    },\n",
      "    \"temperature\": {\n",
      "      \"dtype\": \"float64\",\n",
      "      \"null_count\": 0,\n",
      "      \"null_percentage\": 0.0,\n",
      "      \"unique_count\": 645,\n",
      "      \"min\": 32.59,\n",
      "      \"max\": 52.69,\n",
      "      \"mean\": 37.051716,\n",
      "      \"median\": 37.02\n",
      "    },\n",
      "    \"result\": {\n",
      "      \"dtype\": \"float64\",\n",
      "      \"null_count\": 0,\n",
      "      \"null_percentage\": 0.0,\n",
      "      \"unique_count\": 1219,\n",
      "      \"min\": 45.71,\n",
      "      \"max\": 79.53,\n",
      "      \"mean\": 55.518344,\n",
      "      \"median\": 55.48\n",
      "    },\n",
      "    \"status\": {\n",
      "      \"dtype\": \"object\",\n",
      "      \"null_count\": 0,\n",
      "      \"null_percentage\": 0.0,\n",
      "      \"unique_count\": 4\n",
      "    },\n",
      "    \"notes\": {\n",
      "      \"dtype\": \"object\",\n",
      "      \"null_count\": 510,\n",
      "      \"null_percentage\": 10.2,\n",
      "      \"unique_count\": 3828\n",
      "    }\n",
      "  },\n",
      "  \"example_rows\": [\n",
      "    {\n",
      "      \"experiment_id\": 1013,\n",
      "      \"sample_date\": \"2024-06-01T00:00:00\",\n",
      "      \"sample_name\": \"Epsilon\",\n",
      "      \"temperature\": 37.4,\n",
      "      \"result\": 55.41,\n",
      "      \"status\": \"FAILURE\",\n",
      "      \"notes\": \"Analysis recorded a strong correlation with catalyst concentration under high shear conditions. The assay estimated batch-to-batch variability across replicate samples. The assay noted consistent thermal stability during the late phase of the run.\"\n",
      "    },\n",
      "    {\n",
      "      \"experiment_id\": 1016,\n",
      "      \"sample_date\": \"2025-01-04T00:00:00\",\n",
      "      \"sample_name\": \"Alpha\",\n",
      "      \"temperature\": 38.26,\n",
      "      \"result\": 60.33,\n",
      "      \"status\": \"SUCCESS\",\n",
      "      \"notes\": null\n",
      "    },\n",
      "    {\n",
      "      \"experiment_id\": 1018,\n",
      "      \"sample_date\": \"2025-07-13T00:00:00\",\n",
      "      \"sample_name\": \"Sigma\",\n",
      "      \"temperature\": 36.83,\n",
      "      \"result\": 54.99,\n",
      "      \"status\": \"PENDING\",\n",
      "      \"notes\": \"Analysis detected unexpected precipitation across replicate samples. The assay estimated elevated background noise across replicate samples.\"\n",
      "    }\n",
      "  ],\n",
      "  \"memory_usage_mb\": 1.65\n",
      "}\n",
      "‚úì Summary: The dataset \"labverse_experiments_5k.xlsx\" contains 5,000 rows of experimental data aimed at analyzing various lab samples, with key columns including \"experiment_id\" (unique integer identifiers), \"sample_date\" (datetime values), \"sample_name\" (categorical identifiers), \"temperature\" (float values ranging from 32.59 to 52.69¬∞C), \"result\" (float values between 45.71 and 79.53), \"status\" (categorical with four unique statuses), and \"notes\" (textual observations, with some missing entries). This structured dataset facilitates the evaluation of experimental outcomes and conditions, providing insights into sample performance and variability across different experiments.\n",
      "‚úì Stored embedding for labverse_experiments_5k.xlsx with ID: file_labverse_experiments_5k_xlsx_-881105053954230797\n",
      "  File ID: file_labverse_experiments_5k_xlsx_-881105053954230797\n",
      "  Embedding dimension: 1536\n",
      "  Summary length: 673 chars\n",
      "  Stored in Pinecone: True\n"
     ]
    }
   ],
   "source": [
    "# Generate and store embedding\n",
    "print(\"üî¢ EMBEDDING INFO:\")\n",
    "embedding_info = ingestion.embed_and_store(metadata)\n",
    "print(f\"  File ID: {embedding_info['file_id']}\")\n",
    "print(f\"  Embedding dimension: {embedding_info['embedding_dim']}\")\n",
    "print(f\"  Summary length: {embedding_info['context_length']} chars\")\n",
    "print(f\"  Stored in Pinecone: {embedding_info['stored_in_pinecone']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b62247d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory path/to/your/data/directory does not exist\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "‚úì Successfully processed: 0 files\n",
      "‚úó Failed: 0 files\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Process an entire directory\n",
    "# Update this path to your data directory\n",
    "data_directory = \"path/to/your/data/directory\"\n",
    "\n",
    "# Process all files in the directory\n",
    "results = ingestion.process_directory(data_directory, namespace=\"data-files\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "successful = [r for r in results if r['status'] == 'success']\n",
    "failed = [r for r in results if r['status'] == 'failed']\n",
    "\n",
    "print(f\"‚úì Successfully processed: {len(successful)} files\")\n",
    "print(f\"‚úó Failed: {len(failed)} files\")\n",
    "\n",
    "if successful:\n",
    "    print(\"\\nSuccessfully processed files:\")\n",
    "    for result in successful:\n",
    "        meta = result['metadata']\n",
    "        print(f\"  - {meta['filename']}: {meta['row_count']} rows, {meta['column_count']} columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b96882b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of response 5\n",
      "üîç SEARCH RESULTS for: 'files with glucose measurements and timestamps'\n",
      "============================================================\n",
      "\n",
      "1. Match Score: 0.3835\n",
      "   Filename: labverse_experiments_5k.xlsx\n",
      "   Path: /Users/sid/Programming/LabVerse/test-notebooks/../test_dataset/labverse_experiments_5k.xlsx\n",
      "   Columns: experiment_id,sample_date,sample_name,temperature,result,status,notes\n",
      "   Rows: 5000.0\n",
      "   Summary: The dataset \"labverse_experiments_5k.xlsx\" contains 5,000 rows of experimental data aimed at analyzing various lab samples, with key columns including \"experiment_id\" (unique integer identifiers), \"sample_date\" (datetime values), \"sample_name\" (categorical identifiers), \"temperature\" (float values ranging from 32.59 to 52.69¬∞C), \"result\" (float values between 45.71 and 79.53), \"status\" (categorical with four unique statuses), and \"notes\" (textual observations, with some missing entries). This structured dataset facilitates the evaluation of experimental outcomes and conditions, providing insights into sample performance and variability across different experiments.\n",
      "\n",
      "2. Match Score: 0.3472\n",
      "   Filename: N/A\n",
      "   Path: N/A\n",
      "   Columns: N/A\n",
      "   Rows: N/A\n",
      "\n",
      "3. Match Score: 0.2759\n",
      "   Filename: N/A\n",
      "   Path: N/A\n",
      "   Columns: N/A\n",
      "   Rows: N/A\n",
      "\n",
      "4. Match Score: 0.2745\n",
      "   Filename: N/A\n",
      "   Path: N/A\n",
      "   Columns: N/A\n",
      "   Rows: N/A\n",
      "\n",
      "5. Match Score: 0.2735\n",
      "   Filename: N/A\n",
      "   Path: N/A\n",
      "   Columns: N/A\n",
      "   Rows: N/A\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Search for files using semantic search\n",
    "# Query for files that match a description\n",
    "query = \"files with glucose measurements and timestamps\"\n",
    "\n",
    "# Generate embedding for the query\n",
    "query_embedding, _ = ingestion.embedding_service.embed_text(query)\n",
    "\n",
    "# Search Pinecone for similar files\n",
    "similar_files = ingestion.embedding_service.search_similar(query_embedding, top_k=5)\n",
    "\n",
    "print(f\"üîç SEARCH RESULTS for: '{query}'\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, match in enumerate(similar_files, 1):\n",
    "    print(f\"\\n{i}. Match Score: {match['score']:.4f}\")\n",
    "    meta = match['metadata']\n",
    "    print(f\"   Filename: {meta.get('filename', 'N/A')}\")\n",
    "    print(f\"   Path: {meta.get('filepath', 'N/A')}\")\n",
    "    print(f\"   Columns: {meta.get('column_names', 'N/A')}\")\n",
    "    print(f\"   Rows: {meta.get('row_count', 'N/A')}\")\n",
    "    # Show LLM summary if available\n",
    "    if 'llm_summary' in meta:\n",
    "        print(f\"   Summary: {meta['llm_summary']}\")\n",
    "    elif 'summary_preview' in meta:\n",
    "        print(f\"   Preview: {meta['summary_preview']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labverse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
